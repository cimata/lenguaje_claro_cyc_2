{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZxK12Q10/jxVygCIKiT3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cimata/lenguaje_claro_cyc_2/blob/main/Transcribir_audio_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "thG6U6QXLnv6",
        "outputId": "a2298388-a499-46f0-9ef7-b6ceb643a9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-6esdte99\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-6esdte99\n",
            "  Resolved https://github.com/openai/whisper.git to commit 271445b2f24f00f8175c4fb7ae91876f7451dfc1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803557 sha256=c95319ad50063441227e2486d8fafb7435e56d2c63628451614dc9afefd8fb1f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ufhsdnow/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.19.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from deepl) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2024.8.30)\n",
            "Downloading deepl-1.19.1-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: deepl\n",
            "Successfully installed deepl-1.19.1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [59.5 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,107 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,446 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,163 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,610 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,391 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,672 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,319 kB]\n",
            "Fetched 23.6 MB in 4s (5,557 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "59 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install deepl\n",
        "!apt update && apt install -y ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import deepl\n",
        "\n",
        "# Cargar el modelo de Whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Ruta al archivo de audio (ajústala si tu archivo tiene otro nombre)\n",
        "audio_path = \"3.-Los-3-cerditos.m4a\"\n",
        "\n",
        "# Paso 1: Transcribir el audio en español\n",
        "result = model.transcribe(audio_path, language=\"es\")\n",
        "spanish_text = result[\"text\"]\n",
        "print(\"Transcripción en español:\")\n",
        "print(spanish_text)\n",
        "\n",
        "# Paso 2: Traducir el audio al inglés usando Whisper\n",
        "result_translation = model.transcribe(audio_path, task=\"translate\", language=\"es\")\n",
        "english_text = result_translation[\"text\"]\n",
        "print(\"\\nTraducción al inglés:\")\n",
        "print(english_text)\n",
        "\n",
        "# Paso 3: Traducir el texto en inglés a francés usando DeepL\n",
        "# Reemplaza con tu clave de API de DeepL\n",
        "auth_key = \"c630eb67-a96f-48e9-b720-bd8b88ee2320:fx\"\n",
        "translator = deepl.Translator(auth_key)\n",
        "\n",
        "# Traducir la transcripción en inglés a francés\n",
        "french_translation = translator.translate_text(english_text, target_lang=\"FR\")\n",
        "print(\"\\nTraducción al francés:\")\n",
        "print(french_translation.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s72XpTggL2Zk",
        "outputId": "5d656913-7bee-4615-cedb-2da5412ab9fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcripción en español:\n",
            " Los tres cerditos. Había una vez una mamá cerdo que tenía tres lechones. Lamentablemente no tenía suficiente comida para mantenerlos, por lo que los envió a que buscarán su propia suerte. Cuando el primer cerdito salió, se encontró con un hombre que tenía un manojo de paja. El cerdo le dijo, «Señor, por favor, déme esa paja, así puedo construirme una casa». El hombre le dio la paja y el cerdo se construyó una casa con ella. Poco después pasó por allí un lobo feroz, llamó a la puerta y dijo, «Oye, cerdito, déjame entrar». El cerdito respondió, «No, no, no, ni siquiera por el pelo de mi barba. Entonces, soplaré y resoplaré y la casa de ríbaré». Dijo el lobo feroz. Y así lo hizo. Sopló y resopló y derribó la puerta. El cerdito corrió raudamente hacia la casa de su hermano. Su hermano, el segundo cerdito, había conocido un hombre que tenía un manojo de palos. El cerdito le dijo, «Señor, por favor, déme los palos para que pueda construirme una casa». El hombre le dio los palos y el cerdito se construyó una casa con ellos. Estaba sentado en su casa, sintiéndose muy orgulloso, cuando escuchó que llamaban a la puerta. Era su hermano pequeño. «El lobo feroz derribó mi casa. Por favor, puedo quedarme contigo». «Por supuesto, mi casa está hecha de palos. Sin duda es segura». Dijo su hermano. Entonces, el lobo feroz llegó a la casa hecha de palitos y dijo, «Ej, cerdito, déjame entrar». «No, no, no, ni siquiera por el pelo de mi barba». Replicó el cerdito. «Entonces, soplaré y resoplaré y la casa derribaré». Dijo el lobo feroz. Y así lo hizo. Sopló y resopló y derribó la puerta. Los dos cerditos corrieron hacia la casa de su hermano tan rápido como pudieron. El tercer cerdito había conocido a un hombre con una carga deladrillo. El tercer cerdito dijo, «Por favor, señor, déme esos ladrillos para que pueda construir mi una casa». El hombre le dio los ladrillos y el tercer cerdito construyó su casa con ellos. Estaba sentado en su casa cuando sintió que golpeaban a su puerta. Era en sus hermanos. «El lobo feroz derribó nuestras casas. Podemos quedarnos aquí, por favor». «Por supuesto, mi casa está hecha de ladrillos. Si induda es segura, dijo su hermano. Pero el lobo feroz seguramente vendrá por aquí. Hagamos un plan para asegurarnos de que no nos molesten nunca más». Y los tres cerditos elaboraron un plan. Y tal como el cerdito mayor predijo, el lobo feroz llegó a su casa. «El lobo feroz llegó a la casa hecha deladrillo, si dijo. El cerdito, dajame entrar. «No, no, no, ni siquiera por el pelo de mi barba». Le plico el cerdito. «Entonces, soplaré y resoplaré y la casa derribaré». Dijo el lobo feroz. «Y así lo hizo. Sopló y resopló. Y sopló y resopló. Y resopló y sopló. Pero no pudo derribar la puerta. Pronto se dio cuenta de que no podría derribarla soplando y resoplando. Por supuesto, esto hizo que el lobo feroz se enojara. Y así decidió que para atrapar a los cerditos entraría por la chimenea. Pero los tres cerditos sabían lo que planeaba hacer, y habían puesto una tetera con agua sobre el fuego. Y justo cuando el lobo feroz bajaba por la chimenea, quitaron la tapa de la tetera. El vapor caliente llegó hasta el lobo feroz, que con un gran aullido salió disparado hacia arriba. «Ai, a, a, a, o cerditos. Algun día los atraparé. Y yo tan rápido como pudo. Y después de eso, el lobo feroz nunca regresó. Lo tenía mucho miedo a los tres astutos cerditos. Y los cerditos vivieron felices para siempre, juntos en su casa hecha del Adrillo.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Traducción al inglés:\n",
            " The three of them. There was once a mother who had three legs. Unfortunately, she did not have enough food to keep them, so she sent them to find their own luck. When the first little bird came out, she found a man who had a little bird's eye. The bird told him, ''Mr. please, give me that bird, so I can build a house.'' The man gave the bird and the bird built a house with it. A little later, a wolf passed by, he called the door and said, ''Hey, Cerdito, let it enter.'' Cerdito replied, ''No, no, no, no, no, no, no, no, no, no, no,.♪ Then I will supply and supply and the house will be raised.'' He said, ''The wolf, Feroth.'' And so he did it, supply and supply and raised the door. The bird ran rapidly towards the house of his brother. His brother, the second little bird, had known a man who had a little bird's eye. The little bird said, ''Mr. please, give me the sticks so that I can build a house.'' The man gave the sticks and Cerdito built a house with them. He was sitting in his house feeling very proud when he heard that the door was called. He was his little brother. ''The wolf Feroth raised my house, please, can I stay with you?'' Both said, ''Give me the sticks, Mr. Cerdito, you have enough of them!'' Then the wolf Feroth came home and said, '' ''No, no, no, no, no, no, that barrel of the word, it said, lets me bring it close.'' said the wolf, Feroth. And so he did it, he suffered and he suffered and he got up door. The two cerditos ran towards the house of his brother as fast as they could. The third cerdito had known a man with a load of lads. The cerdito said, please, sir, give me those lads so that I can build a house. The man gave the lads and the cerdito built his house with them. He was sitting in his house when he felt that he was going to his door. They were his brothers. The wolf Feroth raised our houses. We can stay here for a while. Of course, my house is made of lads, if not doubted, it is safe. He said his brother. But the wolf Feroth will surely come here. We make a plan to make sure that they will never get us this one more. And the three cerditos worked on a plan. And such as the cerdito, the eldest, said the wolf Feroth arrived at his house. The wolf Feroth came to the house made of lads and said, the cerdito, let me enter. No, no, no, no, no, no. No, no or any for the hair of my beard. I replicated the cerdito. Then I'll replace and replace and the house of the river. He said in the grandfather's house. And so he did it, he slept and he slept, and he slept, and he slept, and he slept, but he could not get up the door. Soon he realized that he could not get up, sleeping and sleeping. Of course, he did it that the grandfather was angry and so he decided that to catch the birds, he would enter the chimney. But the three birds knew what the one was planning to do, and they had put a tent on the fire. And just when the wolf was down by the chimney, he took the cover of the tent. The hot wolf arrived until the wolf was free, and with a great awry, he went out of the shot up to the top. Oh, oh, oh, oh, oh, oh, oh, they would turn to us someday, and I am as fast as I could. And after that, the wolf never returned. He was very afraid of the three bird-bearing birds, and the birds lived happily forever together in his house\n",
            "\n",
            "Traducción al francés:\n",
            " Les trois. Il était une fois une mère qui avait trois jambes. Malheureusement, elle n'avait pas assez de nourriture pour les garder, alors elle les envoya chercher leur chance. Lorsque le premier petit oiseau est sorti, il a trouvé un homme qui avait l'œil d'un petit oiseau. L'oiseau lui dit : \"Monsieur, s'il vous plaît, donnez-moi cet oiseau pour que je puisse construire une maison\". L'homme lui donna l'oiseau et l'oiseau construisit une maison avec. Un peu plus tard, un loup passa par là, il appela la porte et dit : \"Hé, Cerdito, laisse-le entrer. Cerdito répondit, ''Non, non, non, non, non, non, non, non, non, non, non, non''. ''Alors je fournirai et je fournirai et la maison sera élevée.'' Il a dit, ''Le loup, Feroth.'' Et c'est ce qu'il fit, il fournit et fournit et il souleva la porte. L'oiseau courut rapidement vers la maison de son frère. Son frère, le deuxième petit oiseau, avait connu un homme qui avait un œil de petit oiseau. Le petit oiseau dit : \"Monsieur, s'il vous plaît, donnez-moi les bâtons pour que je puisse construire une maison\". L'homme lui donna les bâtons et Cerdito construisit une maison avec. Il était assis dans sa maison et se sentait très fier lorsqu'il entendit qu'on appelait la porte. C'était son petit frère. Le loup Feroth a soulevé ma maison, s'il vous plaît, puis-je rester avec vous ? Les deux dirent : \"Donnez-moi les bâtons, M. Cerdito, vous en avez assez ! Le loup Feroth revint à la maison et dit : \"Non, non, non, non, non, non, ce tonneau du mot, il dit, laisse-moi l'approcher\", dit le loup Feroth. Et c'est ce qu'il fit, il souffrit, il souffrit et il se releva de la porte. Les deux cerditos coururent vers la maison de son frère aussi vite qu'ils le purent. Le troisième cerdito avait connu un homme avec une cargaison de garçons. Le cerdito lui dit : \"S'il vous plaît, monsieur, donnez-moi ces bêtes pour que je puisse construire une maison. L'homme lui donna les billes et le cerdito construisit sa maison avec. Il était assis dans sa maison lorsqu'il sentit qu'il allait à sa porte. C'étaient ses frères. Le loup Feroth a élevé nos maisons. Nous pouvons rester ici pendant un certain temps. Bien sûr, ma maison est faite de gaillards, si on n'en doute pas, elle est sûre. dit son frère. Mais le loup Feroth viendra sûrement ici. Nous élaborons un plan pour nous assurer qu'ils ne nous atteindront plus jamais. Et les trois cerditos élaborèrent un plan. Et tel cerdito, l'aîné, dit que le loup Feroth arriva chez lui. Le loup Feroth arriva à la maison des garçons et dit : Cerdito, laisse-moi entrer. Non, non, non, non, non, non. Non, non ou aucun pour les poils de ma barbe. J'ai reproduit le cerdito. Alors je remplacerai et remplacerai et la maison de la rivière. Il a dit dans la maison du grand-père. Et c'est ce qu'il a fait, il a dormi et dormi, et dormi, et dormi, et dormi, mais il n'a pas pu ouvrir la porte. Bientôt, il se rendit compte qu'il ne pouvait pas se lever, dormant et dormant. Bien sûr, il fit en sorte que le grand-père soit en colère et il décida que pour attraper les oiseaux, il entrerait dans la cheminée. Mais les trois oiseaux savaient ce que le grand-père avait l'intention de faire, et ils avaient mis une tente sur le feu. Au moment où le loup se trouvait près de la cheminée, il s'abrita sous la tente. Le loup chaud arriva jusqu'à ce que le loup soit libre, et avec une grande maladresse, il sortit du coup de feu jusqu'au sommet. Oh, oh, oh, oh, oh, oh, oh, ils se tourneraient vers nous un jour, et je fais aussi vite que je peux. Après cela, le loup ne revint jamais. Il eut très peur des trois oisillons, et les oiseaux vécurent heureux pour toujours dans sa maison\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "5Gv8lDHzQiTB",
        "outputId": "662435d7-965a-485e-c038-adf6c42b17a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.5.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.5.0 gradio-client-1.4.2 huggingface-hub-0.26.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.2 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import whisper\n",
        "import deepl\n",
        "\n",
        "# Cargar el modelo de Whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Configurar la clave de API de DeepL\n",
        "auth_key = \"c630eb67-a96f-48e9-b720-bd8b88ee2320:fx\"\n",
        "translator = deepl.Translator(auth_key)\n",
        "\n",
        "# Función principal que procesa el archivo de audio\n",
        "def transcribe_and_translate(audio_file):\n",
        "    # Paso 1: Transcripción en español\n",
        "    result = model.transcribe(audio_file, language=\"es\")\n",
        "    spanish_text = result[\"text\"]\n",
        "\n",
        "    # Paso 2: Traducción al inglés con Whisper\n",
        "    result_translation = model.transcribe(audio_file, task=\"translate\", language=\"es\")\n",
        "    english_text = result_translation[\"text\"]\n",
        "\n",
        "    # Paso 3: Traducción al francés con DeepL\n",
        "    french_translation = translator.translate_text(english_text, target_lang=\"FR\").text\n",
        "\n",
        "    return spanish_text, english_text, french_translation\n",
        "\n",
        "# Crear la interfaz de Gradio\n",
        "# Removing the 'source' argument from gr.Audio\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe_and_translate,  # La función a llamar\n",
        "    inputs=gr.Audio(type=\"filepath\"),  # Subir archivo de audio\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Transcripción en Español\"),\n",
        "        gr.Textbox(label=\"Traducción al Inglés\"),\n",
        "        gr.Textbox(label=\"Traducción al Francés\"),\n",
        "    ],\n",
        "    title=\"Transcriptor y Traductor de Audio\",\n",
        "    description=\"Sube un archivo de audio en español para obtener la transcripción y traducciones automáticas en inglés y francés.\"\n",
        ")\n",
        "\n",
        "# Iniciar la aplicación\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "8ZVuhylNQ0Pz",
        "outputId": "58792919-932e-4f87-8ec5-4ae8eb6a9ef0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f06e9981fc779da55c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f06e9981fc779da55c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}